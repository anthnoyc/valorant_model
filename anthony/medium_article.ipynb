{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6ce0d47a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import time\n",
    "import datetime as dt\n",
    "import re\n",
    "from dateutil import parser as dtparse   # pip install python-dateutil"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f09f402b",
   "metadata": {},
   "source": [
    "GAP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7db3d6c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_rows', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "da73c66f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests, re, pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "HEADERS = {\"User-Agent\": \"Mozilla/5.0\"}\n",
    "\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ maps 1â€¯&â€¯2 resolver â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "def first_two_games(base_url: str):\n",
    "    \"\"\"\n",
    "    Returns [(game_id, map_name)] for Mapâ€¯1 and Mapâ€¯2.\n",
    "\n",
    "    â€¢ looks ONLY at the first <div class=\"vm-stats-gamesnav\">   (top nav bar)\n",
    "    â€¢ skips the disabled â€œAllâ€ tab (index 0)\n",
    "    â€¢ collects items 1 and 2   â†’ Mapâ€¯1, Mapâ€¯2\n",
    "    \"\"\"\n",
    "    soup   = BeautifulSoup(requests.get(base_url, headers=HEADERS).text,\n",
    "                           \"html.parser\")\n",
    "\n",
    "    top_nav = soup.select_one(\".vm-stats-gamesnav\")     # top bar only\n",
    "    items   = top_nav.select(\".js-map-switch\")[1:3]     # skip â€˜Allâ€™, take 2\n",
    "\n",
    "    games = []\n",
    "    for div in items:\n",
    "        gid  = div[\"data-game-id\"]\n",
    "        text = \" \".join(div.stripped_strings)           # \"1 Haven\"\n",
    "        map_name = text.split(maxsplit=1)[1]            # \"Haven\"\n",
    "        games.append((gid, map_name))\n",
    "    return games\n",
    "\n",
    "\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ oneâ€‘map scraper â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "def scrape_one_map(match_url: str, game_id: str, map_name: str) -> pd.DataFrame:\n",
    "    base_path = match_url.split(\"?\", 1)[0]                      # keep slug\n",
    "    html      = requests.get(f\"{base_path}/?game={game_id}&tab=overview\",\n",
    "                             headers=HEADERS).text\n",
    "    soup      = BeautifulSoup(html, \"html.parser\")\n",
    "\n",
    "    # â”€â”€ locate the specific map block â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "    block = soup.find(\"div\", class_=\"vm-stats-game\", attrs={\"data-game-id\": game_id})\n",
    "    if not block:\n",
    "        raise ValueError(f\"Could not find stats for gameâ€¯ID {game_id}\")\n",
    "\n",
    "    # header with score / winner\n",
    "    header = block.select_one(\".vm-stats-game-header\")\n",
    "    teams  = [t.text.strip() for t in header.select(\".team-name\")]\n",
    "    scores = [int(s.text)     for s in header.select(\".score\")]\n",
    "    winner = teams[0] if scores and scores[0] > scores[1] else (\n",
    "             teams[1] if scores and scores[1] > scores[0] else None)\n",
    "    # â”€â”€ two team tables inside this block â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "\n",
    "    # â”€â”€ iterate over the first two visible tables (TeamÂ A, TeamÂ B) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "    tables = block.select(\"table.wf-table-inset\")[:2]\n",
    "\n",
    "    records = []\n",
    "    for tbl_idx, tbl in enumerate(tables):          # tbl_idx = 0 or 1\n",
    "        for r in tbl.select(\"tbody tr\"):\n",
    "            name_tag  = r.select_one(\".text-of\")\n",
    "            agent_tag = r.select_one(\".mod-agents img\")\n",
    "            team_tag = header.select(\".team-name\")[tbl_idx].text.strip()\n",
    "            if not (name_tag and agent_tag):\n",
    "                continue\n",
    "\n",
    "            def both(node):\n",
    "                span = node.select_one(\".side.mod-both\") or node.select_one(\".mod-both\")\n",
    "                return span.text.strip() if span else \"\"\n",
    "\n",
    "            records.append({\n",
    "                \"Map\"        : map_name,\n",
    "                \"Team\"       : r.select_one(\".ge-text-light\").text.strip(),\n",
    "                \"Player\"     : name_tag.text.strip(),\n",
    "                \"Agent\"      : agent_tag[\"alt\"].strip(),\n",
    "                \"ACS\"        : both(r.select(\"td.mod-stat\")[1]),\n",
    "                \"Kills\"      : both(r.select_one(\".mod-vlr-kills\")),\n",
    "                \"Deaths\"     : both(r.select_one(\".mod-vlr-deaths\")),\n",
    "                \"Assists\"    : both(r.select_one(\".mod-vlr-assists\")),\n",
    "                \"FirstKills\" : both(r.select_one(\".mod-fb\")),\n",
    "                \"Winner\"     : winner == team_tag,  # â†â€¯flag by table index\n",
    "                \"MapScore\"   : f\"{scores[0]}â€‘{scores[1]}\" if scores else \"\"\n",
    "            })\n",
    "\n",
    "    return pd.DataFrame(records)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1ab1de50",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_match_date(match_url: str):\n",
    "    soup = BeautifulSoup(requests.get(match_url, headers=HEADERS).text,\n",
    "                         \"html.parser\")\n",
    "\n",
    "    tag = soup.select_one(\".match-header-date .moment-tz-convert\")\n",
    "    if not tag:\n",
    "        return None\n",
    "\n",
    "    raw = (tag.get(\"data-utc-ts\") or \"\").strip()\n",
    "    if raw.isdigit():                          # Unix timestamp\n",
    "        return dt.datetime.utcfromtimestamp(int(raw)).date()\n",
    "\n",
    "    # try ISO or other common formats\n",
    "    for candidate in (raw, tag.text.strip()):\n",
    "        try:\n",
    "            return dtparse.parse(candidate, fuzzy=True).date()\n",
    "        except (ValueError, TypeError):\n",
    "            continue\n",
    "    return None\n",
    "\n",
    "def vlr_first_two_maps(match_url: str) -> pd.DataFrame:\n",
    "    match_date = extract_match_date(match_url)   # â† new robust helper\n",
    "\n",
    "    game_info  = first_two_games(match_url)      # [('195773','Haven'), ('195774','Split')]\n",
    "    dfs = [\n",
    "        scrape_one_map(match_url, gid, name).assign(Date=match_date)\n",
    "        for gid, name in game_info\n",
    "    ]\n",
    "    return pd.concat(dfs, ignore_index=True)\n",
    "\n",
    "\n",
    "def scrape_data(df, match_urls):\n",
    "    for i in range(len(match_urls)):\n",
    "        url = match_urls[i]\n",
    "        df2 = vlr_first_two_maps(url)\n",
    "        df2[\"Match\"] = i\n",
    "        df = pd.concat([df, df2], ignore_index=True)\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8a4b0180",
   "metadata": {},
   "outputs": [],
   "source": [
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ 1. grab ALL match URLs for an event â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "def event_match_urls(event_url: str) -> list[str]:\n",
    "    \"\"\"\n",
    "    Returns a list of absolute match URLs from an event page such as\n",
    "    https://www.vlr.gg/event/matches/2276/... .\n",
    "    Ignores duplicates and scrapes only completed / scheduled matches.\n",
    "    \"\"\"\n",
    "    html  = requests.get(event_url, headers=HEADERS).text\n",
    "    soup  = BeautifulSoup(html, \"html.parser\")\n",
    "\n",
    "    links = set()\n",
    "    for a in soup.select(\"a\"):                      # every anchor on the page\n",
    "        href = a.get(\"href\", \"\")\n",
    "        if re.match(r\"^/\\d+/\", href):               # /427995/... pattern\n",
    "            links.add(\"https://www.vlr.gg\" + href.split(\"?\")[0])  # keep slug\n",
    "\n",
    "    return sorted(links)                            # ðŸ”¢ deterministic order\n",
    "\n",
    "\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ 2. scrape event endâ€‘toâ€‘end â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "def scrape_event_first_two_maps(event_url: str,\n",
    "                                pause_sec: float = 1.0) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    For every match listed on the event page:\n",
    "        â€¢ scrape Mapâ€¯1 & Mapâ€¯2 stats (+ outcome, date, etc.)\n",
    "    Returns one big tidy DataFrame.\n",
    "    \"\"\"\n",
    "    all_matches = event_match_urls(event_url)\n",
    "    print(f\"Found {len(all_matches)} matches\")\n",
    "\n",
    "    big_df = pd.DataFrame()\n",
    "    for i, m_url in enumerate(all_matches, 1):\n",
    "        try:\n",
    "            df = vlr_first_two_maps(m_url)\n",
    "            df[\"MatchID\"] = i\n",
    "            df[\"MatchURL\"] = m_url\n",
    "            big_df = pd.concat([big_df, df], ignore_index=True)\n",
    "            print(f\"[{i}/{len(all_matches)}]  âœ…  scraped {m_url}\")\n",
    "        except Exception as e:\n",
    "            print(f\"[{i}/{len(all_matches)}]  âš ï¸  {m_url}  -> {e}\")\n",
    "        time.sleep(pause_sec)                       # be polite\n",
    "    return big_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2225136c",
   "metadata": {},
   "outputs": [],
   "source": [
    "match_urls = [ \n",
    "    \"https://www.vlr.gg/482509/team-liquid-vs-natus-vincere-champions-tour-2025-emea-stage-1-playoffs-lr2/?game=all&tab=overview\",\n",
    "              \n",
    "    \n",
    "]\n",
    "\n",
    "test = pd.DataFrame()\n",
    "test = scrape_data(test, match_urls)\n",
    "test.head()\n",
    "test.to_csv(\"test.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0d9659b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# emea_url = (\"https://www.vlr.gg/event/matches/2380/champions-tour-2025-emea-stage-1/?series_id=all\")\n",
    "# emea_df = scrape_event_first_two_maps(emea_url)\n",
    "# apac_url = (\"https://www.vlr.gg/event/matches/2379/champions-tour-2025-pacific-stage-1/?series_id=all\")\n",
    "# apac_df = scrape_event_first_two_maps(apac_url)\n",
    "# emea_df.to_csv(\"emea.csv\", index=False)\n",
    "# apac_df.to_csv(\"apac.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "658c7894",
   "metadata": {},
   "outputs": [],
   "source": [
    "# cn_url = (\"https://www.vlr.gg/event/matches/2359/champions-tour-2025-china-stage-1/?series_id=all\")\n",
    "# cn_df = scrape_event_first_two_maps(cn_url)\n",
    "# amer_url = (\"https://www.vlr.gg/event/matches/2347/champions-tour-2025-americas-stage-1/?series_id=all\")\n",
    "# amer_df = scrape_event_first_two_maps(amer_url)\n",
    "# cn_df.to_csv(\"cn.csv\", index=False)\n",
    "# amer_df.to_csv(\"amer.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
